{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306c6faa",
   "metadata": {},
   "source": [
    "# EarthArXiv Harvester\n",
    "1. Run reference resolver to match bibcodes newly created since last week's harvest, and update the existing data file\n",
    "2. Query the Crossref API to harvest all data by EarthArXiv DOI prefix\n",
    "4. Compare API-harvested data with existing excel file; check for new records to be ingested, and metadata changes on records that already have a bibcode\n",
    "5. Add new records and insert the metadata changes to the existing data file\n",
    "6. Run the pyingest serializer to generate tagged format files: one for newly harvested records, one for metadata updates\n",
    "7. Sftp the tag files into the adsx EarthArXiv directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habanero import Crossref\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyingest.serializers.classic import Tagged\n",
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Set the master_file for data harvest\n",
    "excel_file = \"eartharxiv_data.xlsx\"\n",
    "master_file = pd.read_excel(excel_file)\n",
    "master_data = pd.DataFrame(master_file)\n",
    "\n",
    "# Get today's date in the desired format\n",
    "today_date = datetime.datetime.now().strftime(\"%y%m%d\")\n",
    "\n",
    "# Use the today_date variable to name your files\n",
    "harvest_tagged_output = f\"{today_date}_eaarx_harvest.tag\"\n",
    "metadata_tagged_output = f\"{today_date}_eaarx_updates.tag\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97879d29",
   "metadata": {},
   "source": [
    "## Resolve for newly added bibcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file into a DataFrame\n",
    "\n",
    "# Get rows with a bibcode value\n",
    "rows_with_bibcode = master_data[master_data['preprint_bib'] != '...................']\n",
    "count_with_bibcode = len(rows_with_bibcode)\n",
    "print(f\"Rows with a bibcode: {count_with_bibcode}\")\n",
    "\n",
    "# Get rows with no bibcode\n",
    "rows_with_no_bibcode = master_data[master_data['preprint_bib'] == '...................']\n",
    "count_no_bibcode = len(rows_with_no_bibcode)\n",
    "print(f\"Rows with no bibcode: {count_no_bibcode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab19d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the 'preprint_bib' column condition\n",
    "filtered_df = master_data[master_data['preprint_bib'] == '...................']\n",
    "\n",
    "# Isolate the DOIs and drop all the papers that have no DOIs (drop null values)\n",
    "dois = filtered_df['preprint_doi'].dropna()\n",
    "\n",
    "# Convert it from a DataFrame to a list\n",
    "doi_list = dois.to_list()\n",
    "print(f\"Detected {len(doi_list)} DOIs to search.\")\n",
    "\n",
    "# --- API REQUEST ---\n",
    "token = \"pHazHxvHjPVPAcotvj7DIijROZXUjG5vXa2OaCQO\"\n",
    "url = \"https://api.adsabs.harvard.edu/v1/search/query?\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for doi in doi_list:\n",
    "    query = f\"doi:{doi}\"\n",
    "    params = {\"q\": query, \"fl\": \"doi,bibcode\", \"rows\": 1}\n",
    "    headers = {'Authorization': 'Bearer ' + token}\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    \n",
    "    # Check for HTTP errors\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"HTTP error: {e}\")\n",
    "        continue  # Skip to the next iteration\n",
    "\n",
    "    # Check if response content is empty\n",
    "    if response.content:\n",
    "        try:\n",
    "            from_solr = response.json()\n",
    "            if from_solr.get('response'):\n",
    "                num_docs = from_solr['response'].get('numFound', 0)\n",
    "                if num_docs > 0:\n",
    "                    for doc in from_solr['response']['docs']:\n",
    "                        result = (doc['bibcode'], doc['doi'][0])\n",
    "                        print(result)\n",
    "                        data.append(result)\n",
    "        except json.JSONDecodeError as json_err:\n",
    "            print(f\"JSON decoding error: {json_err}\")\n",
    "            continue  # Skip to the next iteration\n",
    "    else:\n",
    "        print(\"Empty response\")\n",
    "        continue  # Skip to the next iteration\n",
    "\n",
    "# Create a DataFrame from the API results\n",
    "dois_matched = pd.DataFrame(data, columns=['preprint_bib', 'preprint_doi'])\n",
    "\n",
    "# Merge the results with the original DataFrame\n",
    "# Use a left join to bring in the bibcodes for rows that match on 'preprint_doi'\n",
    "merged = df.merge(dois_matched, on='preprint_doi', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Update 'preprint_bib' only where the current value is '...................'\n",
    "# and there's a new value from the API\n",
    "merged.loc[merged['preprint_bib'] == '...................', 'preprint_bib'] = merged['preprint_bib_new']\n",
    "\n",
    "# Drop the temporary '_new' column created by the merge\n",
    "merged = merged.drop(columns=['preprint_bib_new'])\n",
    "\n",
    "# Export merged data\n",
    "merged.to_excel(excel_file, index=False)\n",
    "print(f\"Merged {len(dois_matched)} matching bibcodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7adb5-33fd-4c1f-99fb-8dc765c81d2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Harvest from CrossRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Crossref()\n",
    "doi_prefix = '10.31223'\n",
    "res = cr.prefixes(ids = doi_prefix, works = True, cursor = \"*\", limit = 200)\n",
    "\n",
    "records = []\n",
    "for entry in res:\n",
    "    for item in entry['message']['items']:\n",
    "\n",
    "        def format_authors(authors_data):\n",
    "            formatted_authors = []\n",
    "            for author in authors_data:\n",
    "                last_name = author.get('family', '')\n",
    "                first_name = author.get('given', '')\n",
    "                if last_name and first_name:\n",
    "                    formatted_authors.append(f\"{last_name}, {first_name}\")\n",
    "                elif last_name and not first_name:\n",
    "                    formatted_authors.append(last_name)\n",
    "            return \"; \".join(formatted_authors)\n",
    "        \n",
    "        def format_affs(authors_data):\n",
    "            formatted_affs = []\n",
    "            for author in authors_data:\n",
    "                aff = author.get('affiliation', '')\n",
    "                orcid = author.get('ORCID', '').lstrip(\"http://orcid.org/\")\n",
    "                formatted_aff = \"\"\n",
    "                if aff:\n",
    "                    formatted_aff = f'{aff}'\n",
    "                if orcid:\n",
    "                    formatted_aff += f'<ID system=\\\"ORCID\\\">{orcid}</ID>'\n",
    "                formatted_affs.append(formatted_aff)\n",
    "            return \"; \".join(formatted_affs)\n",
    "\n",
    "        authors_data = item.get('author', [])\n",
    "        authors = format_authors(authors_data)\n",
    "        affiliations = format_affs(authors_data)\n",
    "        title = item.get('title', '')[0]\n",
    "        group_title = item.get('group-title', '')\n",
    "        abstract = item.get('abstract', '').replace(\"<jats:p>\", \"\").replace(\"</jats:p>\", \"\")\n",
    "        preprint_doi = item.get('DOI', '')\n",
    "        url = item.get('resource', {}).get('primary', {}).get('URL', '')\n",
    "        \n",
    "        links = \"\"\n",
    "        if preprint_doi:\n",
    "            links += f\"DOI: {preprint_doi}\"\n",
    "        if url:\n",
    "            links += f\"; ELECTR: {url}\"\n",
    "        links = links.lstrip(\"; \").rstrip(\"/\")\n",
    "        \n",
    "        pubdate = \"\"\n",
    "        if \"published\" in item and \"date-parts\" in item[\"published\"]:\n",
    "            date_parts = item[\"published\"][\"date-parts\"]\n",
    "            if date_parts:\n",
    "                year, month, day = date_parts[0]\n",
    "                pubdate = f\"{year}/{month:02d}/{day:02d}\"\n",
    "            \n",
    "        article_doi = \"\"\n",
    "        if \"relation\" in item and \"is-preprint-of\" in item[\"relation\"]:\n",
    "            is_preprint_of = item[\"relation\"][\"is-preprint-of\"]\n",
    "            if isinstance(is_preprint_of, list) and len(is_preprint_of) > 0:\n",
    "                article_doi = (is_preprint_of[0][\"id\"]).lstrip(\"https://doi.org/\").lstrip(\"x.doi.org/\")\n",
    "    \n",
    "        wd_abs = abstract == \"Abstract removed when submission was withdrawn.\"\n",
    "        if title != \"\":\n",
    "            r = {\n",
    "                \"authors\": \"; \".join(authors.split(\"; \")),\n",
    "                \"affiliations\": \"; \".join(affiliations.split(\"; \")),\n",
    "                \"pubdate\":pubdate,\n",
    "                \"title\": title,\n",
    "                \"properties\": links,\n",
    "                \"abstract\": abstract,\n",
    "                \"keywords\": group_title,\n",
    "                \"preprint_doi\": preprint_doi,\n",
    "                \"article_doi\": article_doi,\n",
    "                \"source\": \"CrossRef\",\n",
    "                \"preprint_bib\": \"...................\",\n",
    "                \"processed\": \"withdrawn\" if wd_abs else str(today_date)\n",
    "            }\n",
    "            records.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6df417",
   "metadata": {},
   "source": [
    "## Curation: Add new records and metadata changes to data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73847f-e388-42ca-b24f-82fbd0943bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of preprint_dois already harvested\n",
    "master_file = pd.read_excel(excel_file)\n",
    "master_data = pd.DataFrame(master_file)\n",
    "exclusions = set(master_file['preprint_doi'].tolist())\n",
    "\n",
    "# Create a list of the new records based on new preprint_dois\n",
    "records_to_add = []\n",
    "metadata_updates = []\n",
    "\n",
    "def clean_value(value):\n",
    "    \"\"\"Helper function to clean NaN values by converting them to empty strings.\"\"\"\n",
    "    if pd.isna(value) or value is None:\n",
    "        return \"\"\n",
    "    return str(value)\n",
    "\n",
    "for r in records:\n",
    "    \n",
    "    eaarx_doi = r['preprint_doi']\n",
    "\n",
    "    # If the DOI does not yet exist in the master file, append the whole record for the ingest list\n",
    "    if eaarx_doi not in exclusions:\n",
    "        records_to_add.append(r)\n",
    "\n",
    "    # If the DOI exists, check each metadata field for changes from the master file\n",
    "    elif eaarx_doi in exclusions:\n",
    "        # Get the corresponding row in the master file for comparison\n",
    "        master_row = master_data[master_data['preprint_doi'] == eaarx_doi].iloc[0]\n",
    "\n",
    "        updated_fields = {}\n",
    "        \n",
    "        if clean_value(master_row['preprint_bib']) != \"withdrawn\" and clean_value(master_row['preprint_bib']) != \"bad\":\n",
    "            # Compare each relevant field, cleaning NaN values\n",
    "            if clean_value(r['authors']) != clean_value(master_row['authors']):\n",
    "                updated_fields['authors'] = {'old': clean_value(master_row['authors']), 'new': clean_value(r['authors'])}\n",
    "            if clean_value(r['affiliations']) != clean_value(master_row['affiliations']):\n",
    "                updated_fields['affiliations'] = {'old': clean_value(master_row['affiliations']), 'new': clean_value(r['affiliations'])}\n",
    "            if clean_value(r['title']) != clean_value(master_row['title']):\n",
    "                updated_fields['title'] = {'old': clean_value(master_row['title']), 'new': clean_value(r['title'])}\n",
    "            if clean_value(r['pubdate']) != clean_value(master_row['pubdate']):\n",
    "                updated_fields['pubdate'] = {'old': clean_value(master_row['pubdate']), 'new': clean_value(r['pubdate'])}\n",
    "            if clean_value(r['abstract']) != clean_value(master_row['abstract']):\n",
    "                updated_fields['abstract'] = {'old': clean_value(master_row['abstract']), 'new': clean_value(r['abstract'])}\n",
    "            if clean_value(r['article_doi']) != clean_value(master_row['article_doi']):\n",
    "                updated_fields['article_doi'] = {'old': clean_value(master_row['article_doi']), 'new': clean_value(r['article_doi'])}\n",
    "    \n",
    "            # If there are any updates, append to metadata_updates\n",
    "            if updated_fields:\n",
    "                r = {\n",
    "                    'preprint_doi': eaarx_doi,\n",
    "                    'updates': updated_fields\n",
    "                }\n",
    "                print(r, '\\n')\n",
    "                metadata_updates.append(r)\n",
    "\n",
    "# Now metadata_updates will contain all the changes made to existing records\n",
    "print(f\"Harvester detected {len(records_to_add)} new records.\")\n",
    "print(f\"Metadata changes detected for {len(metadata_updates)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4e514-ac02-420c-86d6-29a973326643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the master_data DataFrame with the metadata changes\n",
    "for update in metadata_updates:\n",
    "    preprint_doi = update['preprint_doi']\n",
    "    \n",
    "    # Extract the 'new' values from the updates\n",
    "    new_values = {key: value['new'] for key, value in update['updates'].items()}\n",
    "    \n",
    "    # Locate the row in the master_data where the preprint_doi matches\n",
    "    master_data.loc[master_data['preprint_doi'] == preprint_doi, list(new_values.keys())] = list(new_values.values())\n",
    "\n",
    "# Merge the new records with the original data and remove duplicates based on preprint_doi\n",
    "new_records_df = pd.DataFrame(records_to_add)\n",
    "merged_df = pd.concat([master_data, new_records_df], ignore_index=True)\n",
    "merged_df = merged_df.drop_duplicates(subset=[\"preprint_doi\"])\n",
    "\n",
    "# Write the merged DataFrame back to the master excel file\n",
    "merged_df.to_excel(excel_file, index=False)\n",
    "print(f\"Updated existing metadata and added {len(records_to_add)} new records to {excel_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75025b49",
   "metadata": {},
   "source": [
    "## Curation: Generate tagged format records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639786d-cf69-446b-978b-936e5c9107c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyingest.serializers.classic import Tagged\n",
    "\n",
    "### Function to Process Records and Generate Tagged Format ###\n",
    "def process_records(records, output_file):\n",
    "    # Pyingest Serializer - Transform records into tagged format\n",
    "    with open(output_file, 'a') as outputfp:\n",
    "        for record in records:\n",
    "            try:\n",
    "                serializer = Tagged()\n",
    "                serializer.write(record, outputfp)\n",
    "            except Exception as e:\n",
    "                print(f\"Serializer failed for record: {record}, Error: {e}\")\n",
    "    \n",
    "    # Post-processing: Perform &amp; to & replacement\n",
    "    with open(output_file, 'r') as file:\n",
    "        data = file.read()\n",
    "        data = re.sub(r'&amp;', '&', data)\n",
    "    \n",
    "    # Write the modified content back to the file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(data)\n",
    "\n",
    "### TAG FILE FOR NEW RECORDS HARVEST ###\n",
    "# Prepare the records for tagging\n",
    "add_records = pd.DataFrame(records_to_add)\n",
    "new_records = []\n",
    "for index, row in add_records.iterrows():\n",
    "    if row[\"abstract\"] != \"Abstract removed when submission was withdrawn.\":  # Check abstract condition\n",
    "        r = {\n",
    "            \"authors\": [] if pd.isna(row[\"authors\"]) else row[\"authors\"].split(\"; \"),\n",
    "            \"affiliations\": [] if pd.isna(row[\"affiliations\"]) else row[\"affiliations\"].split(\"; \"),\n",
    "            \"pubdate\": \"\" if pd.isna(row[\"pubdate\"]) else row[\"pubdate\"],\n",
    "            \"title\": \"\" if pd.isna(row[\"title\"]) else row[\"title\"],\n",
    "            \"properties\": [] if pd.isna(row[\"properties\"]) else row[\"properties\"].split(\"; \"),\n",
    "            \"abstract\": \"\" if pd.isna(row[\"abstract\"]) else row[\"abstract\"],\n",
    "            \"keywords\": \"\" if pd.isna(row[\"keywords\"]) else row[\"keywords\"],\n",
    "            \"comments\": \"\" if pd.isna(row[\"article_doi\"]) else f'Published article doi: {row[\"article_doi\"]}',\n",
    "            \"source\": \"CrossRef\"\n",
    "        }\n",
    "        new_records.append(r)\n",
    "\n",
    "# Process and generate the tagged file for new records\n",
    "process_records(new_records, harvest_tagged_output)\n",
    "print(f\"Saved {len(new_records)} new records to {harvest_tagged_output}\")\n",
    "\n",
    "# Prepare the metadata updates for tagging\n",
    "updated_records = []\n",
    "for update in metadata_updates:\n",
    "    preprint_doi = update['preprint_doi']\n",
    "    \n",
    "    # Find the corresponding entry in master_data to get preprint_bib and old article doi\n",
    "    master_row = master_data.loc[master_data['preprint_doi'] == preprint_doi]\n",
    "    preprint_bib = master_row['preprint_bib'].values\n",
    "    old_article_doi = master_row['article_doi'].values[0] if not master_row.empty else None\n",
    "\n",
    "    # Extract the new article DOI and preprint DOI\n",
    "    new_article_doi = update[\"updates\"].get(\"article_doi\", {}).get(\"new\", \"\").lstrip(\"x.doi.org/\")\n",
    "    new_preprint_doi = update[\"updates\"].get(\"preprint_doi\", {}).get(\"new\", \"\").lstrip(\"x.doi.org/\")\n",
    "    \n",
    "    # Skip updates that only change pubdate or where new preprint DOI matches the old\n",
    "    if (len(update[\"updates\"]) == 1 and 'pubdate' in update[\"updates\"]) or \\\n",
    "       (old_article_doi and str(old_article_doi).lstrip(\"x.doi.org/\") == str(new_preprint_doi).lstrip(\"x.doi.org/\")):\n",
    "        continue  # Skip this update\n",
    "\n",
    "    # Create the comment conditionally\n",
    "    comments = f'Published article doi: {new_article_doi}' if new_article_doi else \"\"\n",
    "\n",
    "    r = {\n",
    "        \"authors\": update[\"updates\"].get(\"authors\", {}).get(\"new\", \"\"),\n",
    "        \"affiliations\": update[\"updates\"].get(\"affiliations\", {}).get(\"new\", \"\"),\n",
    "        \"title\": update[\"updates\"].get(\"title\", {}).get(\"new\", \"\"),\n",
    "        \"properties\": update[\"updates\"].get(\"properties\", {}).get(\"new\", \"\"),\n",
    "        \"abstract\": update[\"updates\"].get(\"abstract\", {}).get(\"new\", \"\"),\n",
    "        \"keywords\": update[\"updates\"].get(\"keywords\", {}).get(\"new\", \"\"),\n",
    "        \"comments\": comments,\n",
    "        \"bibcode\": preprint_bib[0] if len(preprint_bib) > 0 else \"\",  # Get preprint_bib or default to empty string\n",
    "    }\n",
    "    \n",
    "    updated_records.append(r)\n",
    "\n",
    "# Process and generate the tagged file for metadata updates\n",
    "process_records(updated_records, metadata_tagged_output)\n",
    "print(f\"Saved {len(updated_records)} updated records to {metadata_tagged_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af6521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## (*Not in use*) Resolve published article DOIs for matching bibcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Import new excel sheet\n",
    "# excel_file = \"eartharxiv_data.xlsx\"\n",
    "# path = \"/Users/sao/Documents/Python-Projects/EarthArXiv/\"\n",
    "# df = pd.read_excel(path + excel_file)\n",
    "\n",
    "# # Filter the DataFrame based on conditions for 'article_doi' and 'article_bib'\n",
    "# filtered_df = df[(~df['article_doi'].isnull()) & (df['article_bib'].isnull())]\n",
    "\n",
    "# # Isolate the DOIs and drop all the rows that have no DOIs (drop null values)\n",
    "# dois = filtered_df['article_doi'].dropna()\n",
    "\n",
    "# # Convert it from a DataFrame to a list\n",
    "# doi_list = dois.to_list()\n",
    "# print(\"Filtered list has\", len(doi_list), \"DOIs to search.\")\n",
    "\n",
    "# # --- API REQUEST --- \n",
    "# token = \"pHazHxvHjPVPAcotvj7DIijROZXUjG5vXa2OaCQO\"\n",
    "# url = \"https://api.adsabs.harvard.edu/v1/search/query?\"\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for doi in doi_list:\n",
    "#     query = f\"doi:{doi}\"\n",
    "    \n",
    "#     params = {\"q\": query, \"fl\": \"doi,bibcode\", \"rows\": 1}\n",
    "#     headers = {'Authorization': 'Bearer ' + token}\n",
    "#     response = requests.get(url, params=params, headers=headers)\n",
    "    \n",
    "#     # Check for HTTP errors\n",
    "#     try:\n",
    "#         response.raise_for_status()\n",
    "#     except requests.HTTPError as e:\n",
    "#         print(f\"HTTP error: {e}\")\n",
    "#         continue  # Skip to the next iteration\n",
    "\n",
    "#     # Check if response content is empty\n",
    "#     if response.content:\n",
    "#         try:\n",
    "#             from_solr = response.json()\n",
    "#             if from_solr.get('response'):\n",
    "#                 num_docs = from_solr['response'].get('numFound', 0)\n",
    "#                 if num_docs > 0:\n",
    "#                     for doc in from_solr['response']['docs']:\n",
    "#                         result = (doc['bibcode'], doc['doi'][0])\n",
    "#                         print(result)\n",
    "#                         data.append(result)\n",
    "#         except json.JSONDecodeError as json_err:\n",
    "#             print(f\"JSON decoding error: {json_err}\")\n",
    "#             continue  # Skip to the next iteration\n",
    "#     else:\n",
    "#         print(\"Empty response\")\n",
    "#         continue  # Skip to the next iteration\n",
    "\n",
    "# dois_matched = pd.DataFrame(data, columns=['article_bib', 'article_doi'])\n",
    "# merged = df.merge(dois_matched, left_on='article_doi', right_on='article_doi', how='left')\n",
    "\n",
    "# # Export merged data\n",
    "# merged.to_excel(path + excel_file, index=False)\n",
    "# print(f\"Merged {len(dois_matched)} bibcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7ff75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
